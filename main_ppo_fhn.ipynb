{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4803b-cf1d-46dc-a4be-6b2ec3bc3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "\n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from solver_sdmd_torch_gpu2 import KoopmanNNTorch, KoopmanSolverTorch\n",
    "from sde_coefficients_estimator import SDECoefficientEstimator\n",
    "from numpy import random as rrr\n",
    "from time import time\n",
    "from koopman_bandit import KoopmanBandit\n",
    "import joblib\n",
    "from collections import namedtuple, deque\n",
    "import math\n",
    "import random as rnd\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "from fhn_system_2d import simulate_trajectory\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318b09e-0c04-42ac-a2ea-43fe31a3ce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (torch.__version__, torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print (torch.cuda.get_device_name())\n",
    "#device= 'cpu'\n",
    "device= 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee00847-786e-4ad4-9283-b729dc0abb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "beta = 1\n",
    "delta = 0.25\n",
    "epsilon = 0.05\n",
    "\n",
    "\n",
    "a1= 1/3\n",
    "b1= 0.5\n",
    "b2= 0\n",
    "\n",
    "# DX= 0.2\n",
    "# DY= 0.2\n",
    "DX= 0.05**2\n",
    "DY = 0.05**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07700c-f37a-4a8b-a5dd-a79e2e832b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_trajectory(x0, y0, T):\n",
    "    \"\"\"\n",
    "    Produce SDE trajectory starting at (x0, y0) over T steps.\n",
    "    This function delegates the simulation to duffing_system_2d.simulate_trajectory.\n",
    "    \n",
    "    Args:\n",
    "        x0: Initial x-coordinate (position)\n",
    "        y0: Initial y-coordinate (velocity)\n",
    "        T: Total simulation time steps\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (data_matrix_single, lag_time)\n",
    "    \"\"\"\n",
    "    return simulate_trajectory(x0, y0, T, h=1e-4, n_steps=100, beta= beta, delta= delta, \n",
    "                        epsilon= epsilon, a1=a1, b1=b1, b2=b2, DX= DX, DY= DY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7814b-7e9f-4d20-ac43-5a0ef5c35467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(seconds):\n",
    "            hours = int(seconds // 3600)\n",
    "            minutes = int((seconds % 3600) // 60)\n",
    "            seconds = int(seconds % 60)\n",
    "            return f\"{hours:02d}h:{minutes:02d}m:{seconds:02d}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f793e97-158d-4b01-bea3-138298f8a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ee_reward (trajectory,next_initial_point):\n",
    "    ee_epsilon= 0.1\n",
    "    kernel = stats.gaussian_kde(trajectory.squeeze().T)\n",
    "    mean_kernel= np.mean (kernel(trajectory.squeeze().T))\n",
    "    normalized_p= kernel (next_initial_point)/mean_kernel\n",
    "    ee_reward= 1.0/(normalized_p + ee_epsilon)\n",
    "    return ee_reward\n",
    "\n",
    "def do_koopman_step (state, action, k_grid= 4, state_len= 10, chunk_len= 100):\n",
    "    chunk_list= []\n",
    "    T= chunk_len\n",
    "    action_x= action//k_grid\n",
    "    action_y= action%k_grid\n",
    "    for ii in arange(state_len-1):\n",
    "        \n",
    "        x0_single= state[0, ii]\n",
    "        y0_single=  state[1, ii]\n",
    "   \n",
    "        data_matrix_single, lag_time= get_single_trajectory (x0_single, y0_single, T)\n",
    "        chunk_list.append (data_matrix_single)\n",
    "    x_borders= linspace (-3,3, k_grid+1)\n",
    "    y_borders= linspace (-4,4, k_grid+1)\n",
    "    x_lo= x_borders[action_x]\n",
    "    x_hi= x_borders[action_x+1]\n",
    "    y_lo= y_borders[action_y]\n",
    "    y_hi= y_borders[action_y+1]\n",
    "    x0_single= rrr.uniform (x_lo, x_hi)\n",
    "    y0_single= rrr.uniform (y_lo, y_hi)\n",
    "    data_matrix_single, lag_time= get_single_trajectory (x0_single, y0_single, T)\n",
    "    trajectory= hstack (chunk_list)\n",
    "    chunk_list.append (data_matrix_single)\n",
    "    data_matrix_single= hstack (chunk_list)\n",
    "    ee_reward= get_ee_reward (trajectory, [x0_single, y0_single])\n",
    "    #print ('EE reward:' , ee_reward)\n",
    "    #Extract data_X and data_Y from the data matrix\n",
    "    data_X = data_matrix_single[:, :-1, :]\n",
    "    data_Y = data_matrix_single[:, 1:, :]\n",
    "    print(f\"Shape of data_X: {data_X.shape}\")\n",
    "    print(f\"Shape of data_Y: {data_Y.shape}\")\n",
    "    \n",
    "    # Reshape data_X and data_Y into a single column\n",
    "    X = data_X.reshape(-1, data_X.shape[2])  # 2D features\n",
    "    Y = data_Y.reshape(-1, data_X.shape[2])  # 2D targets\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of Y: {Y.shape}\")\n",
    "    \n",
    "    # Separate data into two parts: train and validation\n",
    "    len_all = X.shape[0]\n",
    "    data_x_train = X[:int(0.7*len_all)]\n",
    "    data_x_valid = X[int(0.7*len_all)+1:]\n",
    "    \n",
    "    data_y_train = Y[:int(0.7*len_all)]\n",
    "    data_y_valid = Y[int(0.7*len_all)+1:]\n",
    "    \n",
    "    data_train = [data_x_train, data_y_train]\n",
    "    data_valid = [data_x_valid, data_y_valid]\n",
    "    \n",
    "    print(data_x_train.shape)\n",
    "    \n",
    "    checkpoint_file= f'example_{system}_dqn_ckpt3.torch'\n",
    "    basis_function = KoopmanNNTorch(input_size= 2, layer_sizes=[20], n_psi_train=17).to(device)  # basis number would be 20\n",
    "    \n",
    "    \n",
    "    solver = KoopmanSolverTorch(dic=basis_function, # Replace 'koopman_nn' by 'dic' if you use the original solver_edmdvar\n",
    "                           target_dim=np.shape(data_x_train)[-1],\n",
    "                                                       reg=0.1,  checkpoint_file= checkpoint_file, fnn_checkpoint_file= f'example_{system}_fnn_dqn1.torch', \n",
    "                                a_b_file= f'sde_coefficients_example_{system}_dqn1.jbl', \n",
    "                            generator_batch_size= 2, fnn_batch_size= 32, delta_t= lag_time)\n",
    "    solver.build_with_generator(\n",
    "    data_train=data_train,\n",
    "    data_valid=data_valid,\n",
    "    epochs=4,\n",
    "    batch_size=256,\n",
    "    lr=1e-5,\n",
    "    log_interval=10,\n",
    "    lr_decay_factor=.8\n",
    "    )\n",
    "    consistency= 0\n",
    "    for ii in arange (len (solver.eigenvalues)):\n",
    "        phi_x= solver.eigenfunctions (X)[ii]\n",
    "        phi_y=  solver.eigenfunctions (Y)[ii]\n",
    "        lmbd=  solver.eigenvalues[ii]\n",
    "        consistency= consistency + la.norm (phi_y- lmbd*phi_x)**2\n",
    "    print (consistency)\n",
    "    reward= 16- consistency+  0.15*ee_reward\n",
    "    next_state= hstack ([state[:, 1:], expand_dims (array ([x0_single, y0_single]), axis= 1)])\n",
    "    return next_state, reward\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c49828c-a9ea-4d74-8e4c-1577b6fb5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Data Store ####################################################\n",
    "class PPOMemory():\n",
    "    \"\"\"\n",
    "    Memory for PPO\n",
    "    \"\"\"\n",
    "    def  __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.actions= []\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "        self.vals = []\n",
    "        self.dones = []\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        ## suppose n_states=20 and batch_size = 4\n",
    "        n_states = len(self.states)\n",
    "        ##n_states should be always greater than batch_size\n",
    "        ## batch_start is the starting index of every batch\n",
    "        ## eg:   array([ 0,  4,  8, 12, 16]))\n",
    "        batch_start = np.arange(0, n_states, self.batch_size) \n",
    "        ## random shuffling if indexes\n",
    "        # eg: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        ## eg: array([12, 17,  6,  7, 10, 11, 15, 13, 18,  9,  8,  4,  3,  0,  2,  5, 14,19,  1, 16])\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "        ## eg: [array([12, 17,  6,  7]),array([10, 11, 15, 13]),array([18,  9,  8,  4]),array([3, 0, 2, 5]),array([14, 19,  1, 16])]\n",
    "        return np.array(self.states),np.array(self.actions),\\\n",
    "               np.array(self.action_probs),np.array(self.vals),np.array(self.rewards),\\\n",
    "               np.array(self.dones),batches\n",
    "    \n",
    "       \n",
    "    \n",
    "\n",
    "    def store_memory(self,state,action,action_prob,val,reward,done):\n",
    "        self.states.append(state.ravel())\n",
    "        self.actions.append(action)\n",
    "        self.action_probs.append(action_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.vals.append(val)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.actions= []\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "        self.vals = []\n",
    "        self.dones = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76034f84-fa4e-4bfa-baae-df3ee9b04e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## initialize actor network and critic network\n",
    "\n",
    "\n",
    "class ActorNwk(nn.Module):\n",
    "    def __init__(self,input_dim,out_dim,\n",
    "                 adam_lr,\n",
    "                 chekpoint_file,\n",
    "                 hidden1_dim=256,\n",
    "                 hidden2_dim=256\n",
    "                 ):\n",
    "        super(ActorNwk, self).__init__()\n",
    "\n",
    "        self.actor_nwk = nn.Sequential(\n",
    "            nn.Linear(*input_dim,hidden1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1_dim,hidden2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2_dim,out_dim),  \n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.checkpoint_file = chekpoint_file\n",
    "        self.optimizer = torch.optim.Adam(params=self.actor_nwk.parameters(),lr=adam_lr)\n",
    "\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    \n",
    "    def forward(self,state):\n",
    "        out = self.actor_nwk(state)\n",
    "        dist = Categorical(out)\n",
    "        return dist\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "############################### Crirtic Network ######################################\n",
    "\n",
    "class CriticNwk(nn.Module):\n",
    "    def __init__(self,input_dim,\n",
    "                 adam_lr,\n",
    "                 chekpoint_file,\n",
    "                 hidden1_dim=256,\n",
    "                 hidden2_dim=256\n",
    "                 ):\n",
    "        super(CriticNwk, self).__init__()\n",
    "\n",
    "        self.critic_nwk = nn.Sequential(\n",
    "            nn.Linear(*input_dim,hidden1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1_dim,hidden2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2_dim,1),  \n",
    "   \n",
    "        )\n",
    "\n",
    "        self.checkpoint_file = chekpoint_file\n",
    "        self.optimizer = torch.optim.Adam(params=self.critic_nwk.parameters(),lr=adam_lr)\n",
    "\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    \n",
    "    def forward(self,state):\n",
    "        out = self.critic_nwk(state)\n",
    "        return out\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c2361-6eab-4057-8225-0f8d46282870",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initilaize an Agent will will be able to train the model\n",
    "\n",
    "############################# Agent ########################################3\n",
    "\n",
    "## agent\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, gamma, policy_clip,lamda, adam_lr,\n",
    "                 n_epochs, batch_size, state_dim, action_dim):\n",
    "        \n",
    "        self.gamma = gamma \n",
    "        self.policy_clip = policy_clip\n",
    "        self.lamda  = lamda\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.actor = ActorNwk(input_dim=state_dim,out_dim=action_dim,adam_lr=adam_lr,chekpoint_file='tmp/actor')\n",
    "        self.critic = CriticNwk(input_dim=state_dim,adam_lr=adam_lr,chekpoint_file='tmp/ctitic')\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "    def store_data(self,state,action,action_prob,val,reward,done):\n",
    "        self.memory.store_memory(state,action,action_prob,val,reward,done)\n",
    "       \n",
    "\n",
    "    def save_models(self):\n",
    "        print('... Saving Models ......')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "    \n",
    "    def load_models(self):\n",
    "        print('... Loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float64).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        ## sample the output action from a categorical distribution of predicted actions\n",
    "        action = dist.sample()\n",
    "        probs = torch.squeeze(dist.log_prob(action)).item()\n",
    "        action = torch.squeeze(action).item()\n",
    "\n",
    "        ## value from critic model\n",
    "        value = self.critic(state)\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    \n",
    "    def calculate_advanatage(self,reward_arr,value_arr,dones_arr):\n",
    "        time_steps = len(reward_arr)\n",
    "        advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "        for t in range(0,time_steps-1):\n",
    "            discount = 1\n",
    "            running_advantage = 0\n",
    "            for k in range(t,time_steps-1):\n",
    "                if int(dones_arr[k]) == 1:\n",
    "                    running_advantage += reward_arr[k] - value_arr[k]\n",
    "                else:\n",
    "                \n",
    "                    running_advantage += reward_arr[k] + (self.gamma*value_arr[k+1]) - value_arr[k]\n",
    "\n",
    "                running_advantage = discount * running_advantage\n",
    "                # running_advantage += discount*(reward_arr[k] + self.gamma*value_arr[k+1]*(1-int(dones_arr[k])) - value_arr[k])\n",
    "                discount *= self.gamma * self.lamda\n",
    "            \n",
    "            advantage[t] = running_advantage\n",
    "        advantage = torch.tensor(advantage).to(self.actor.device)\n",
    "        return advantage\n",
    "    \n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "\n",
    "            ## initially all will be empty arrays\n",
    "            state_arr, action_arr, old_prob_arr, value_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "            \n",
    "            advantage_arr = self.calculate_advanatage(reward_arr,value_arr,dones_arr)\n",
    "            values = torch.tensor(value_arr).to(self.actor.device)\n",
    "\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.double).to(self.actor.device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = torch.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = torch.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage_arr[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage_arr[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage_arr[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d72ff-6700-43ff-baa3-dcf34e042fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_grid= 32\n",
    "state_len= 12\n",
    "chunk_len= 100\n",
    "n_actions = k_grid**2\n",
    "# Get the number of state observations\n",
    "\n",
    "n_observations = state_len*2\n",
    "system= 'fhn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fedac-e89b-4117-b3e4-144a3b5df375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('tmp'):\n",
    "    os.makedirs('tmp')\n",
    "\n",
    "\n",
    "N = 20\n",
    "batch_size = 5\n",
    "n_epochs = 4\n",
    "alpha = 0.0003\n",
    "agent = Agent(state_dim=(n_observations, ),\n",
    "              action_dim=n_actions, \n",
    "              batch_size=batch_size,\n",
    "              n_epochs=n_epochs,\n",
    "              policy_clip=0.2,\n",
    "              gamma=0.99,lamda=0.95, \n",
    "              adam_lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b33746-b3f2-4c10-abfd-0b9abfe45fe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "best_score = 0 # env.unwrapped.reward_range[0]\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "num_steps= 10001\n",
    "num_episodes= 1\n",
    "step_count = 0\n",
    "reward_hist= []\n",
    "start_time= time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    initial_state = 3 * (np.random.uniform(size=(2, state_len)) - 1)\n",
    "    current_state = initial_state\n",
    "    # current_state,info = env.reset()\n",
    "    # terminated,truncated = False,False\n",
    "    done = False\n",
    "    score = 0\n",
    "    for t in arange(num_steps):\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        \n",
    "        # Calculate estimated time remaining\n",
    "        if t > 0:\n",
    "            avg_time_per_step = elapsed_time / t\n",
    "            remaining_steps = num_steps - t\n",
    "            estimated_time_remaining = avg_time_per_step * remaining_steps\n",
    "        else:\n",
    "            estimated_time_remaining = 0\n",
    "        action, prob, val = agent.choose_action(current_state.ravel())\n",
    "        #next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        observation, reward = do_koopman_step(current_state, action, k_grid=k_grid)\n",
    "        next_state= observation#.ravel()\n",
    "        #done = 1 if (terminated or truncated) else 0\n",
    "        step_count += 1\n",
    "        score += reward\n",
    "        agent.store_data(current_state, action, prob, val, reward, done)\n",
    "        reward_hist.append([current_state, action, next_state, reward])\n",
    "        if step_count % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        current_state = next_state\n",
    "    # Write detailed status to file\n",
    "        with open(f'ppo_state_{system}_test.txt', 'w') as f:\n",
    "            f.write(f\"System: {system}\\n\")\n",
    "            f.write(f\"Current RL step: {int(t)}/{num_steps} ({(t/num_steps)*100:.1f}%)\\n\")\n",
    "            f.write(f\"Elapsed time: {format_time(elapsed_time)}\\n\")\n",
    "            f.write(f\"Estimated time remaining: {format_time(estimated_time_remaining)}\\n\")\n",
    "         \n",
    "    # Periodically save results\n",
    "        if (t % 100 == 0):\n",
    "            joblib.dump(\n",
    "                reward_hist,\n",
    "                f'ppo_history{k_grid}x{k_grid}_{num_steps}steps_example_{system}_test.jbl'\n",
    "            )\n",
    "            print(f\"Step {t}/{num_steps} completed. System: {system}. Elapsed: {format_time(elapsed_time)}\")\n",
    "    \n",
    "    \n",
    "    agent.save_models()\n",
    "    print('episode', i_episode, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "            'time_steps', step_count, 'learning_steps', learn_iters)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8913ab5-1018-4046-b537-beb1c3fd295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump (reward_hist, 'dqn_history64x64_15000steps_example_final.jbl')\n",
    "filename = f'ppo_history{k_grid}x{k_grid}_{num_steps}steps_example_{system}_final.jbl'\n",
    "joblib.dump(\n",
    "    reward_hist,\n",
    "    filename\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env001",
   "language": "python",
   "name": "torch_env001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
